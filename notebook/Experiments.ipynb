{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1212a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "191f356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "def do_sum(dataframe, group_cols, counted, agg_name):\n",
    "    gp = dataframe[group_cols + [counted]].groupby(group_cols)[counted].sum().reset_index().rename(columns={counted: agg_name})\n",
    "    dataframe = dataframe.merge(gp, on=group_cols, how='left')\n",
    "    return dataframe\n",
    "\n",
    "def reduce_mem_usage(dataframe):\n",
    "    m_start = dataframe.memory_usage().sum() / 1024 ** 2\n",
    "    for col in dataframe.columns:\n",
    "        col_type = dataframe[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = dataframe[col].min()\n",
    "            c_max = dataframe[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.int64)\n",
    "            elif str(col_type)[:5] == 'float':\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.float32)\n",
    "                else:\n",
    "                    dataframe[col] = dataframe[col].astype(np.float64)\n",
    "\n",
    "    m_end = dataframe.memory_usage().sum() / 1024 ** 2\n",
    "    return dataframe\n",
    "\n",
    "nan_as_category = True\n",
    "\n",
    "\n",
    "def risk_groupanizer(dataframe, column_names, target_val=1, upper_limit_ratio=8.2, lower_limit_ratio=8.2):\n",
    "    # one-hot encoder killer :-)\n",
    "    all_cols = dataframe.columns\n",
    "    for col in column_names:\n",
    "\n",
    "        temp_df = dataframe.groupby([col] + ['TARGET'])[['SK_ID_CURR']].count().reset_index()\n",
    "        temp_df['ratio%'] = round(temp_df['SK_ID_CURR']*100/temp_df.groupby([col])['SK_ID_CURR'].transform('sum'), 1)\n",
    "        col_groups_high_risk = temp_df[(temp_df['TARGET'] == target_val) &\n",
    "                                       (temp_df['ratio%'] >= upper_limit_ratio)][col].tolist()\n",
    "        col_groups_low_risk = temp_df[(temp_df['TARGET'] == target_val) &\n",
    "                                      (lower_limit_ratio >= temp_df['ratio%'])][col].tolist()\n",
    "        if upper_limit_ratio != lower_limit_ratio:\n",
    "            col_groups_medium_risk = temp_df[(temp_df['TARGET'] == target_val) &\n",
    "                (upper_limit_ratio > temp_df['ratio%']) & (temp_df['ratio%'] > lower_limit_ratio)][col].tolist()\n",
    "\n",
    "            for risk, col_groups in zip(['_high_risk', '_medium_risk', '_low_risk'],\n",
    "                                        [col_groups_high_risk, col_groups_medium_risk, col_groups_low_risk]):\n",
    "                dataframe[col + risk] = [1 if val in col_groups else 0 for val in dataframe[col].values]\n",
    "        else:\n",
    "            for risk, col_groups in zip(['_high_risk', '_low_risk'], [col_groups_high_risk, col_groups_low_risk]):\n",
    "                dataframe[col + risk] = [1 if val in col_groups else 0 for val in dataframe[col].values]\n",
    "        if dataframe[col].dtype == 'O' or dataframe[col].dtype == 'object':\n",
    "            dataframe.drop(col, axis=1, inplace=True)\n",
    "    return dataframe, list(set(dataframe.columns).difference(set(all_cols)))\n",
    "\n",
    "\n",
    "def ligthgbm_feature_selection(dataframe, index_cols, auc_limit=0.7):\n",
    "    dataframe = dataframe.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '_', x))\n",
    "    clf = LGBMClassifier(random_state=0)\n",
    "    train_df = dataframe[dataframe['TARGET'].notnull()]\n",
    "    X = dataframe.drop('TARGET', axis=1)\n",
    "    y = dataframe['TARGET']\n",
    "    train_columns = [col for col in X.columns if col not in index_cols]\n",
    "\n",
    "    max_auc_score = 1\n",
    "    best_cols = []\n",
    "    while max_auc_score > auc_limit:\n",
    "        train_columns = [col for col in train_columns if col not in best_cols]\n",
    "        clf.fit(X[train_columns], y)\n",
    "        feats_imp = pd.Series(clf.feature_importances_, index=train_columns)\n",
    "        max_auc_score = roc_auc_score(y, clf.predict_proba(X[train_columns])[:, 1])\n",
    "        bad_cols = feats_imp[feats_imp <12].index.tolist()\n",
    "\n",
    "    dataframe.drop(train_columns, axis=1, inplace=True)\n",
    "    return dataframe, bad_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58dfcb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1(df, n_folds = 5):\n",
    "    \n",
    "    features = df[df['TARGET'].notnull()]\n",
    "    test_features = df[df['TARGET'].isnull()]\n",
    "    #features, test_features = train.align(test_features, join = 'inner', axis = 1)\n",
    "    # Extract the ids\n",
    "    train_ids = features['SK_ID_CURR']\n",
    "    test_ids = test_features['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = features['TARGET']\n",
    "    \n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    test_features = test_features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    \n",
    "    \n",
    "    ''' One Hot Encoding\n",
    "    if encoding == 'ohe':\n",
    "        features = pd.get_dummies(features)\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "        \n",
    "        # Align the dataframes by the columns\n",
    "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
    "        \n",
    "        # No categorical indices to record\n",
    "        cat_indices = 'auto'\n",
    "    \n",
    "    # Integer label encoding\n",
    "    elif encoding == 'le':\n",
    "        \n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "        \n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "        \n",
    "        # Iterate through each column\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == 'object':\n",
    "                # Map the categorical features to integers\n",
    "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
    "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
    "\n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "    \n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "        \n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)'''\n",
    "    \n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    \n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    \n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "        \n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "        \n",
    "        # Create the model\n",
    "        model = LGBMClassifier(n_estimators=2266, objective = 'binary', \n",
    "                                   class_weight = 'balanced', learning_rate = 0.01, \n",
    "                                   reg_alpha = 0.02, reg_lambda = 0.9, \n",
    "                                   subsample = 0.86667, n_jobs = -1, random_state = 500)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'])\n",
    "        \n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "        \n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
    "        \n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "        \n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "        \n",
    "    # Make the submission dataframe\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
    "    \n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "    \n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "    \n",
    "    # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "    \n",
    "    # Dataframe of validation scores\n",
    "    ''' metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores})''' \n",
    "    fi_drop=feature_importances[feature_importances['importance']< 12]\n",
    "    dropfeat=fi_drop['feature'].tolist()\n",
    "    return submission, dropfeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c128195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Transformation\n",
    "def group(df_to_agg, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
    "    agg_df = df_to_agg.groupby(aggregate_by).agg(aggregations)\n",
    "    agg_df.columns = pd.Index(['{}{}_{}'.format(prefix, e[0], e[1].upper())\n",
    "                               for e in agg_df.columns.tolist()])\n",
    "    return agg_df.reset_index()\n",
    "\n",
    "def group_and_merge(df_to_agg, df_to_merge, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
    "    agg_df = group(df_to_agg, prefix, aggregations, aggregate_by= aggregate_by)\n",
    "    return df_to_merge.merge(agg_df, how='left', on= aggregate_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "361f1ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ingestion address\n",
    "\n",
    "\n",
    "train=pd.read_csv(r\"C:\\Users\\aksha\\notebooks\\data\\application_train.csv\")\n",
    "test=pd.read_csv(r\"C:\\Users\\aksha\\notebooks\\data\\application_test.csv\")\n",
    "bureau=pd.read_csv(r\"C:\\Users\\aksha\\notebooks\\data\\bureau.csv\")\n",
    "bureau_bal=pd.read_csv(r\"C:\\Users\\aksha\\notebooks\\data\\bureau_balance.csv\")\n",
    "cc_bal=pd.read_csv(r\"C:\\Users\\aksha\\notebooks\\data\\credit_card_balance.csv\")\n",
    "payments=pd.read_csv(r\"C:\\Users\\aksha\\notebooks\\data\\installments_payments.csv\")\n",
    "cash=pd.read_csv(r\"C:\\Users\\aksha\\notebooks\\data\\POS_CASH_balance.csv\")\n",
    "prev_app=pd.read_csv(r\"C:\\Users\\aksha\\notebooks\\data\\previous_application.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e59af642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_columns(df, threshold = 75):\n",
    "    # Calculate missing stats for train and test (remember to calculate a percent!)\n",
    "    df_miss= pd.DataFrame(df.isnull().sum())\n",
    "    df_miss['percent'] = 100 * df_miss[0] / len(df)\n",
    "    \n",
    "    '''test_miss = pd.DataFrame(test.isnull().sum())\n",
    "    test_miss['percent'] = 100 * test_miss[0] / len(test)\n",
    "    \n",
    "    # list of missing columns for train and test'''\n",
    "    missing_columns = list(df_miss.index[df_miss['percent'] > threshold])\n",
    "    s = list(df_miss.index[df_miss['percent'] < threshold])\n",
    "    ''' missing_test_columns = list(test_miss.index[test_miss['percent'] > threshold])\n",
    "    \n",
    "    # Combine the two lists together\n",
    "    missing_columns = list(set(missing_train_columns + missing_test_columns))\n",
    "    '''\n",
    "    # Print information\n",
    "    print('There are %d columns with greater than %d%% missing values.' % (len(missing_columns), threshold))\n",
    "    \n",
    "    # Drop the missing columns and return\n",
    "    df = df.drop(columns = missing_columns)\n",
    "    #test = test.drop(columns = missing_columns)\n",
    "    \n",
    "    for m in [df]:\n",
    "        for col in s:\n",
    "            if m[col].isnull().sum()>0:\n",
    "                if m[col].dtype=='object' or m[col].dtype=='category':\n",
    "                    md= df[col].mode()\n",
    "                    m[col].fillna(md, inplace=True)\n",
    "            \n",
    "    \n",
    "    '''for m in [test]:\n",
    "        for col in missing_test_columns:\n",
    "            med = test[col].median()\n",
    "            m[col].fillna(med, inplace=True)'''\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to calculate missing values by column# Funct \n",
    "def missing_values_table(df, print_info = False):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        if print_info:\n",
    "            # Print some summary information\n",
    "            print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "                \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "                  \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66c5450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def application(df1, df2):\n",
    "    '''df = pd.read_csv(r'../input/home-credit-default-risk/application_train.csv')\n",
    "    test_df = pd.read_csv(r'../input/home-credit-default-risk/application_test.csv')\n",
    "    df = df.append(test_df).reset_index()'''\n",
    "\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "    # general cleaning procedures\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    df = df[df['AMT_INCOME_TOTAL'] < 20000000] # remove a outlier 117M\n",
    "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True) # set null value\n",
    "    df['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True) # set null value\n",
    "\n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    \n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "\n",
    "    # Flag_document features - count and kurtosis\n",
    "    docs = [f for f in df.columns if 'FLAG_DOC' in f]\n",
    "    df['DOCUMENT_COUNT'] = df[docs].sum(axis=1)\n",
    "    df['NEW_DOC_KURT'] = df[docs].kurtosis(axis=1)\n",
    "\n",
    "    def get_age_label(days_birth):\n",
    "        \"\"\" Return the age group label (int). \"\"\"\n",
    "        age_years = -days_birth / 365\n",
    "        if age_years < 27: return 1\n",
    "        elif age_years < 40: return 2\n",
    "        elif age_years < 50: return 3\n",
    "        elif age_years < 65: return 4\n",
    "        elif age_years < 99: return 5\n",
    "        else: return 0\n",
    "    # Categorical age - based on target=1 plot\n",
    "    df['AGE_RANGE'] = df['DAYS_BIRTH'].apply(lambda x: get_age_label(x))\n",
    "\n",
    "    # New features based on External sources\n",
    "    df['EXT_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
    "    df['EXT_SOURCES_WEIGHTED'] = df.EXT_SOURCE_1 * 2 + df.EXT_SOURCE_2 * 1 + df.EXT_SOURCE_3 * 3\n",
    "    #np.warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n",
    "    for function_name in ['min', 'max', 'mean', 'nanmedian', 'var']:\n",
    "        feature_name = 'EXT_SOURCES_{}'.format(function_name.upper())\n",
    "        df[feature_name] = eval('np.{}'.format(function_name))(\n",
    "            df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)\n",
    "\n",
    "    # Some simple new features (percentages)\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "\n",
    "    # Credit ratios\n",
    "    df['CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
    "    \n",
    "    # Income ratios\n",
    "    df['INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_EMPLOYED']\n",
    "    df['INCOME_TO_BIRTH_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_BIRTH']\n",
    "    \n",
    "    # Time ratios\n",
    "    df['ID_TO_BIRTH_RATIO'] = df['DAYS_ID_PUBLISH'] / df['DAYS_BIRTH']\n",
    "    df['CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\n",
    "    df['CAR_TO_EMPLOYED_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n",
    "    df['PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\n",
    "\n",
    "    # EXT_SOURCE_X FEATURE\n",
    "    df['APPS_EXT_SOURCE_MEAN'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "    df['APPS_EXT_SOURCE_STD'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "    df['APPS_EXT_SOURCE_STD'] = df['APPS_EXT_SOURCE_STD'].fillna(df['APPS_EXT_SOURCE_STD'].mean())\n",
    "    df['APP_SCORE1_TO_BIRTH_RATIO'] = df['EXT_SOURCE_1'] / (df['DAYS_BIRTH'] / 365.25)\n",
    "    df['APP_SCORE2_TO_BIRTH_RATIO'] = df['EXT_SOURCE_2'] / (df['DAYS_BIRTH'] / 365.25)\n",
    "    df['APP_SCORE3_TO_BIRTH_RATIO'] = df['EXT_SOURCE_3'] / (df['DAYS_BIRTH'] / 365.25)\n",
    "    df['APP_SCORE1_TO_EMPLOY_RATIO'] = df['EXT_SOURCE_1'] / (df['DAYS_EMPLOYED'] / 365.25)\n",
    "    df['APP_EXT_SOURCE_2*EXT_SOURCE_3*DAYS_BIRTH'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['DAYS_BIRTH']\n",
    "    df['APP_SCORE1_TO_FAM_CNT_RATIO'] = df['EXT_SOURCE_1'] / df['CNT_FAM_MEMBERS']\n",
    "    df['APP_SCORE1_TO_GOODS_RATIO'] = df['EXT_SOURCE_1'] / df['AMT_GOODS_PRICE']\n",
    "    df['APP_SCORE1_TO_CREDIT_RATIO'] = df['EXT_SOURCE_1'] / df['AMT_CREDIT']\n",
    "    df['APP_SCORE1_TO_SCORE2_RATIO'] = df['EXT_SOURCE_1'] / df['EXT_SOURCE_2']\n",
    "    df['APP_SCORE1_TO_SCORE3_RATIO'] = df['EXT_SOURCE_1'] / df['EXT_SOURCE_3']\n",
    "    df['APP_SCORE2_TO_CREDIT_RATIO'] = df['EXT_SOURCE_2'] / df['AMT_CREDIT']\n",
    "    df['APP_SCORE2_TO_REGION_RATING_RATIO'] = df['EXT_SOURCE_2'] / df['REGION_RATING_CLIENT']\n",
    "    df['APP_SCORE2_TO_CITY_RATING_RATIO'] = df['EXT_SOURCE_2'] / df['REGION_RATING_CLIENT_W_CITY']\n",
    "    df['APP_SCORE2_TO_POP_RATIO'] = df['EXT_SOURCE_2'] / df['REGION_POPULATION_RELATIVE']\n",
    "    df['APP_SCORE2_TO_PHONE_CHANGE_RATIO'] = df['EXT_SOURCE_2'] / df['DAYS_LAST_PHONE_CHANGE']\n",
    "    df['APP_EXT_SOURCE_1*EXT_SOURCE_2'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2']\n",
    "    df['APP_EXT_SOURCE_1*EXT_SOURCE_3'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_3']\n",
    "    df['APP_EXT_SOURCE_2*EXT_SOURCE_3'] = df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
    "    df['APP_EXT_SOURCE_1*DAYS_EMPLOYED'] = df['EXT_SOURCE_1'] * df['DAYS_EMPLOYED']\n",
    "    df['APP_EXT_SOURCE_2*DAYS_EMPLOYED'] = df['EXT_SOURCE_2'] * df['DAYS_EMPLOYED']\n",
    "    df['APP_EXT_SOURCE_3*DAYS_EMPLOYED'] = df['EXT_SOURCE_3'] * df['DAYS_EMPLOYED']\n",
    "\n",
    "    # AMT_INCOME_TOTAL : income\n",
    "    # CNT_FAM_MEMBERS  : the number of family members\n",
    "    df['APPS_GOODS_INCOME_RATIO'] = df['AMT_GOODS_PRICE'] / df['AMT_INCOME_TOTAL']\n",
    "    df['APPS_CNT_FAM_INCOME_RATIO'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    \n",
    "    # DAYS_BIRTH : Client's age in days at the time of application\n",
    "    # DAYS_EMPLOYED : How many days before the application the person started current employment\n",
    "    df['APPS_INCOME_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_EMPLOYED']\n",
    "\n",
    "    # other feature from better than 0.8\n",
    "    df['CREDIT_TO_GOODS_RATIO_2'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
    "    df['APP_AMT_INCOME_TOTAL_12_AMT_ANNUITY_ratio'] = df['AMT_INCOME_TOTAL'] / 12. - df['AMT_ANNUITY']\n",
    "    df['APP_INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_EMPLOYED']\n",
    "    df['APP_DAYS_LAST_PHONE_CHANGE_DAYS_EMPLOYED_ratio'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_EMPLOYED']\n",
    "    df['APP_DAYS_EMPLOYED_DAYS_BIRTH_diff'] = df['DAYS_EMPLOYED'] - df['DAYS_BIRTH']\n",
    "    \n",
    "    # Groupby the client id (SK_ID_CURR), count the number of previous loans, and rename the column\n",
    "    previous_loan_counts = bureau.groupby('SK_ID_CURR', as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU': 'previous_loan_counts'})\n",
    "    #Merging cell\n",
    "    df = df.merge(previous_loan_counts, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "\n",
    "    dropfeatures = ['FLAG_OWN_CAR','OWN_CAR_AGE','FLAG_MOBIL','FLAG_EMP_PHONE','FLAG_WORK_PHONE','FLAG_PHONE','FLAG_EMAIL','HOUR_APPR_PROCESS_START','APARTMENTS_AVG','BASEMENTAREA_AVG','YEARS_BEGINEXPLUATATION_AVG','YEARS_BUILD_AVG','COMMONAREA_AVG','ELEVATORS_AVG','ENTRANCES_AVG','FLOORSMAX_AVG','FLOORSMIN_AVG','LANDAREA_AVG','LIVINGAPARTMENTS_AVG','LIVINGAREA_AVG','NONLIVINGAPARTMENTS_AVG','NONLIVINGAREA_AVG','APARTMENTS_MODE','BASEMENTAREA_MODE','YEARS_BEGINEXPLUATATION_MODE','YEARS_BUILD_MODE','COMMONAREA_MODE','ELEVATORS_MODE','ENTRANCES_MODE','FLOORSMAX_MODE','FLOORSMIN_MODE','LANDAREA_MODE','LIVINGAPARTMENTS_MODE','LIVINGAREA_MODE','NONLIVINGAPARTMENTS_MODE','APARTMENTS_MEDI','BASEMENTAREA_MEDI','YEARS_BEGINEXPLUATATION_MEDI','YEARS_BUILD_MEDI','COMMONAREA_MEDI','ELEVATORS_MEDI','ENTRANCES_MEDI','FLOORSMAX_MEDI','FLOORSMIN_MEDI','LANDAREA_MEDI','LIVINGAPARTMENTS_MEDI','LIVINGAREA_MEDI','NONLIVINGAPARTMENTS_MEDI','NONLIVINGAREA_MEDI','TOTALAREA_MODE','FLAG_DOCUMENT_2','FLAG_DOCUMENT_3','FLAG_DOCUMENT_4','FLAG_DOCUMENT_5','FLAG_DOCUMENT_6','FLAG_DOCUMENT_7','FLAG_DOCUMENT_8','FLAG_DOCUMENT_9','FLAG_DOCUMENT_10','FLAG_DOCUMENT_11','FLAG_DOCUMENT_12','FLAG_DOCUMENT_13','FLAG_DOCUMENT_14','FLAG_DOCUMENT_15','FLAG_DOCUMENT_16','FLAG_DOCUMENT_17','FLAG_DOCUMENT_18','FLAG_DOCUMENT_19','FLAG_DOCUMENT_20','FLAG_DOCUMENT_21']\n",
    "    df=df.drop(columns=dropfeatures)\n",
    "    print('Final shape:', df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9f351a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape: (356250, 239)\n"
     ]
    }
   ],
   "source": [
    "df=application(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62c48b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureaubal(bureau, bb):\n",
    "    # Credit duration and credit/account end date difference\n",
    "    bureau['CREDIT_DURATION'] = -bureau['DAYS_CREDIT'] + bureau['DAYS_CREDIT_ENDDATE']\n",
    "    bureau['ENDDATE_DIF'] = bureau['DAYS_CREDIT_ENDDATE'] - bureau['DAYS_ENDDATE_FACT']\n",
    "    \n",
    "    # Credit to debt ratio and difference\n",
    "    bureau['DEBT_PERCENTAGE'] = bureau['AMT_CREDIT_SUM'] / bureau['AMT_CREDIT_SUM_DEBT']\n",
    "    bureau['DEBT_CREDIT_DIFF'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_DEBT']\n",
    "    bureau['CREDIT_TO_ANNUITY_RATIO'] = bureau['AMT_CREDIT_SUM'] / bureau['AMT_ANNUITY']\n",
    "    bureau['BUREAU_CREDIT_FACT_DIFF'] = bureau['DAYS_CREDIT'] - bureau['DAYS_ENDDATE_FACT']\n",
    "    bureau['BUREAU_CREDIT_ENDDATE_DIFF'] = bureau['DAYS_CREDIT'] - bureau['DAYS_CREDIT_ENDDATE']\n",
    "    bureau['BUREAU_CREDIT_DEBT_RATIO'] = bureau['AMT_CREDIT_SUM_DEBT'] / bureau['AMT_CREDIT_SUM']\n",
    "\n",
    "    # CREDIT_DAY_OVERDUE :\n",
    "    bureau['BUREAU_IS_DPD'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    bureau['BUREAU_IS_DPD_OVER120'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 120 else 0)\n",
    "\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "\n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size', 'mean']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "\n",
    "    #Status of Credit Bureau loan during the month\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "\n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean', 'min'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean', 'max'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean', 'max', 'sum'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean', 'sum'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n",
    "        'SK_ID_BUREAU': ['count'],\n",
    "        'DAYS_ENDDATE_FACT': ['min', 'max', 'mean'],\n",
    "        'ENDDATE_DIF': ['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_FACT_DIFF': ['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_ENDDATE_DIFF': ['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_DEBT_RATIO': ['min', 'max', 'mean'],\n",
    "        'DEBT_CREDIT_DIFF': ['min', 'max', 'mean'],\n",
    "        'BUREAU_IS_DPD': ['mean', 'sum'],\n",
    "        'BUREAU_IS_DPD_OVER120': ['mean', 'sum']\n",
    "        }\n",
    "\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    print('\"Bureau/Bureau Balance\" final shape:', bureau_agg.shape)\n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d433a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Bureau/Bureau Balance\" final shape: (305811, 200)\n"
     ]
    }
   ],
   "source": [
    "bureau = bureaubal(bureau, bureau_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91f81948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 categorical variables\n",
      " \n",
      " []\n"
     ]
    }
   ],
   "source": [
    "catVars = [var for var in bureau.columns if bureau[var].dtype=='object']\n",
    "\n",
    "print('There are {} categorical variables\\n'.format(len(catVars)), '\\n', catVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "351106d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_application(prev):\n",
    "    \n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n",
    "\n",
    "    # Days 365.243 values -> nan\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "\n",
    "    # Feature engineering: ratios and difference\n",
    "    prev['APPLICATION_CREDIT_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n",
    "    prev['CREDIT_TO_ANNUITY_RATIO'] = prev['AMT_CREDIT'] / prev['AMT_ANNUITY']\n",
    "    prev['DOWN_PAYMENT_TO_CREDIT'] = prev['AMT_DOWN_PAYMENT'] / prev['AMT_CREDIT']\n",
    "\n",
    "    # Interest ratio on previous application (simplified)\n",
    "    total_payment = prev['AMT_ANNUITY'] * prev['CNT_PAYMENT']\n",
    "    prev['SIMPLE_INTERESTS'] = (total_payment / prev['AMT_CREDIT'] - 1) / prev['CNT_PAYMENT']\n",
    "\n",
    "    # Days last due difference (scheduled x done)\n",
    "    prev['DAYS_LAST_DUE_DIFF'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_LAST_DUE']\n",
    "\n",
    "    # from off\n",
    "    prev['PREV_GOODS_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_GOODS_PRICE']\n",
    "    prev['PREV_ANNUITY_APPL_RATIO'] = prev['AMT_ANNUITY']/prev['AMT_APPLICATION']\n",
    "    prev['PREV_GOODS_APPL_RATIO'] = prev['AMT_GOODS_PRICE'] / prev['AMT_APPLICATION']\n",
    "\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean', 'sum'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean', 'sum'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean', 'sum'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "        'SK_ID_PREV': ['nunique'],\n",
    "        'DAYS_TERMINATION': ['max'],\n",
    "        'CREDIT_TO_ANNUITY_RATIO': ['mean', 'max'],\n",
    "        'APPLICATION_CREDIT_DIFF': ['min', 'max', 'mean', 'sum'],\n",
    "        'DOWN_PAYMENT_TO_CREDIT': ['mean'],\n",
    "        'PREV_GOODS_DIFF': ['mean', 'max', 'sum'],\n",
    "        'PREV_GOODS_APPL_RATIO': ['mean', 'max'],\n",
    "        'DAYS_LAST_DUE_DIFF': ['mean', 'max', 'sum'],\n",
    "        'SIMPLE_INTERESTS': ['mean', 'max']\n",
    "    }\n",
    "\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "\n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    print('\"Previous Applications\" final shape:', prev_agg.shape)\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d15176f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Previous Applications\" final shape: (338857, 321)\n"
     ]
    }
   ],
   "source": [
    "prev_app = previous_application(prev_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9db90ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 categorical variables\n",
      " \n",
      " []\n"
     ]
    }
   ],
   "source": [
    "catVars = [var for var in prev_app.columns if prev_app[var].dtype=='object']\n",
    "\n",
    "print('There are {} categorical variables\\n'.format(len(catVars)), '\\n', catVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "620a48ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pos_cash(pos):\n",
    "    \n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category=True)\n",
    "\n",
    "    # Flag months with late payment\n",
    "    pos['LATE_PAYMENT'] = pos['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    pos['POS_IS_DPD'] = pos['SK_DPD'].apply(lambda x: 1 if x > 0 else 0) # <-- same with ['LATE_PAYMENT']\n",
    "    pos['POS_IS_DPD_UNDER_120'] = pos['SK_DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n",
    "    pos['POS_IS_DPD_OVER_120'] = pos['SK_DPD'].apply(lambda x: 1 if x >= 120 else 0)\n",
    "\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size', 'min'],\n",
    "        'SK_DPD': ['max', 'mean', 'sum', 'var', 'min'],\n",
    "        'SK_DPD_DEF': ['max', 'mean', 'sum'],\n",
    "        'SK_ID_PREV': ['nunique'],\n",
    "        'LATE_PAYMENT': ['mean'],\n",
    "        'SK_ID_CURR': ['count'],\n",
    "        'CNT_INSTALMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'CNT_INSTALMENT_FUTURE': ['min', 'max', 'mean', 'sum'],\n",
    "        'POS_IS_DPD': ['mean', 'sum'],\n",
    "        'POS_IS_DPD_UNDER_120': ['mean', 'sum'],\n",
    "        'POS_IS_DPD_OVER_120': ['mean', 'sum'],\n",
    "    }\n",
    "\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "\n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "\n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "\n",
    "\n",
    "    sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n",
    "    gp = sort_pos.groupby('SK_ID_PREV')\n",
    "    df_pos = pd.DataFrame()\n",
    "    df_pos['SK_ID_CURR'] = gp['SK_ID_CURR'].first()\n",
    "    df_pos['MONTHS_BALANCE_MAX'] = gp['MONTHS_BALANCE'].max()\n",
    "\n",
    "    # Percentage of previous loans completed and completed before initial term\n",
    "    df_pos['POS_LOAN_COMPLETED_MEAN'] = gp['NAME_CONTRACT_STATUS_Completed'].mean()\n",
    "    df_pos['POS_COMPLETED_BEFORE_MEAN'] = gp['CNT_INSTALMENT'].first() - gp['CNT_INSTALMENT'].last()\n",
    "    df_pos['POS_COMPLETED_BEFORE_MEAN'] = df_pos.apply(lambda x: 1 if x['POS_COMPLETED_BEFORE_MEAN'] > 0 \\\n",
    "                                                                      and x['POS_LOAN_COMPLETED_MEAN'] > 0 else 0, axis=1)\n",
    "    # Number of remaining installments (future installments) and percentage from total\n",
    "    df_pos['POS_REMAINING_INSTALMENTS'] = gp['CNT_INSTALMENT_FUTURE'].last()\n",
    "    df_pos['POS_REMAINING_INSTALMENTS_RATIO'] = gp['CNT_INSTALMENT_FUTURE'].last()/gp['CNT_INSTALMENT'].last()\n",
    "\n",
    "    # Group by SK_ID_CURR and merge\n",
    "    df_gp = df_pos.groupby('SK_ID_CURR').sum().reset_index()\n",
    "    df_gp.drop(['MONTHS_BALANCE_MAX'], axis=1, inplace= True)\n",
    "    pos_agg = pd.merge(pos_agg, df_gp, on= 'SK_ID_CURR', how= 'left')\n",
    "\n",
    "    # Percentage of late payments for the 3 most recent applications\n",
    "    pos = do_sum(pos, ['SK_ID_PREV'], 'LATE_PAYMENT', 'LATE_PAYMENT_SUM')\n",
    "\n",
    "    # Last month of each application\n",
    "    last_month_df = pos.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n",
    "\n",
    "    # Most recent applications (last 3)\n",
    "    sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n",
    "    gp = sort_pos.iloc[last_month_df].groupby('SK_ID_CURR').tail(3)\n",
    "    gp_mean = gp.groupby('SK_ID_CURR').mean().reset_index()\n",
    "    pos_agg = pd.merge(pos_agg, gp_mean[['SK_ID_CURR', 'LATE_PAYMENT_SUM']], on='SK_ID_CURR', how='left')\n",
    "\n",
    "    print('\"Pos-Cash\" balance final shape:', pos_agg.shape) \n",
    "    return pos_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbdba941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Pos-Cash\" balance final shape: (337252, 46)\n"
     ]
    }
   ],
   "source": [
    "cash=pos_cash(cash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00335824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 categorical variables\n",
      " \n",
      " []\n"
     ]
    }
   ],
   "source": [
    "catVars = [var for var in cash.columns if cash[var].dtype=='object']\n",
    "\n",
    "print('There are {} categorical variables\\n'.format(len(catVars)), '\\n', catVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25c016d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def installment(ins):\n",
    "    \n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category=True)\n",
    "\n",
    "    # Group payments and get Payment difference\n",
    "    ins = do_sum(ins, ['SK_ID_PREV', 'NUM_INSTALMENT_NUMBER'], 'AMT_PAYMENT', 'AMT_PAYMENT_GROUPED')\n",
    "    ins['PAYMENT_DIFFERENCE'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT_GROUPED']\n",
    "    ins['PAYMENT_RATIO'] = ins['AMT_INSTALMENT'] / ins['AMT_PAYMENT_GROUPED']\n",
    "    ins['PAID_OVER_AMOUNT'] = ins['AMT_PAYMENT'] - ins['AMT_INSTALMENT']\n",
    "    ins['PAID_OVER'] = (ins['PAID_OVER_AMOUNT'] > 0).astype(int)\n",
    "\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD_diff'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD_diff'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD_diff'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD_diff'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    # Flag late payment\n",
    "    ins['LATE_PAYMENT'] = ins['DBD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    ins['INSTALMENT_PAYMENT_RATIO'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['LATE_PAYMENT_RATIO'] = ins.apply(lambda x: x['INSTALMENT_PAYMENT_RATIO'] if x['LATE_PAYMENT'] == 1 else 0, axis=1)\n",
    "\n",
    "    # Flag late payments that have a significant amount\n",
    "    ins['SIGNIFICANT_LATE_PAYMENT'] = ins['LATE_PAYMENT_RATIO'].apply(lambda x: 1 if x > 0.05 else 0)\n",
    "    \n",
    "    # Flag k threshold late payments\n",
    "    ins['DPD_7'] = ins['DPD'].apply(lambda x: 1 if x >= 7 else 0)\n",
    "    ins['DPD_15'] = ins['DPD'].apply(lambda x: 1 if x >= 15 else 0)\n",
    "\n",
    "    ins['INS_IS_DPD_UNDER_120'] = ins['DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n",
    "    ins['INS_IS_DPD_OVER_120'] = ins['DPD'].apply(lambda x: 1 if (x >= 120) else 0)\n",
    "\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum', 'var'],\n",
    "        'DBD': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum', 'min'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum', 'min'],\n",
    "        'SK_ID_PREV': ['size', 'nunique'],\n",
    "        'PAYMENT_DIFFERENCE': ['mean'],\n",
    "        'PAYMENT_RATIO': ['mean', 'max'],\n",
    "        'LATE_PAYMENT': ['mean', 'sum'],\n",
    "        'SIGNIFICANT_LATE_PAYMENT': ['mean', 'sum'],\n",
    "        'LATE_PAYMENT_RATIO': ['mean'],\n",
    "        'DPD_7': ['mean'],\n",
    "        'DPD_15': ['mean'],\n",
    "        'PAID_OVER': ['mean'],\n",
    "        'DPD_diff':['mean', 'min', 'max'],\n",
    "        'DBD_diff':['mean', 'min', 'max'],\n",
    "        'DAYS_INSTALMENT': ['mean', 'max', 'sum'],\n",
    "        'INS_IS_DPD_UNDER_120': ['mean', 'sum'],\n",
    "        'INS_IS_DPD_OVER_120': ['mean', 'sum']\n",
    "    }\n",
    "\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "\n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "\n",
    "    # from oof (DAYS_ENTRY_PAYMENT)\n",
    "    cond_day = ins['DAYS_ENTRY_PAYMENT'] >= -365\n",
    "    ins_d365_grp = ins[cond_day].groupby('SK_ID_CURR')\n",
    "    ins_d365_agg_dict = {\n",
    "        'SK_ID_CURR': ['count'],\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['mean', 'max', 'sum'],\n",
    "        'DAYS_INSTALMENT': ['mean', 'max', 'sum'],\n",
    "        'AMT_INSTALMENT': ['mean', 'max', 'sum'],\n",
    "        'AMT_PAYMENT': ['mean', 'max', 'sum'],\n",
    "        'PAYMENT_DIFF': ['mean', 'min', 'max', 'sum'],\n",
    "        'PAYMENT_PERC': ['mean', 'max'],\n",
    "        'DPD_diff': ['mean', 'min', 'max'],\n",
    "        'DPD': ['mean', 'sum'],\n",
    "        'INS_IS_DPD_UNDER_120': ['mean', 'sum'],\n",
    "        'INS_IS_DPD_OVER_120': ['mean', 'sum']}\n",
    "\n",
    "    ins_d365_agg = ins_d365_grp.agg(ins_d365_agg_dict)\n",
    "    ins_d365_agg.columns = ['INS_D365' + ('_').join(column).upper() for column in ins_d365_agg.columns.ravel()]\n",
    "\n",
    "    ins_agg = ins_agg.merge(ins_d365_agg, on='SK_ID_CURR', how='left')\n",
    "\n",
    "    print('\"Installments Payments\" final shape:', ins_agg.shape)\n",
    "    return ins_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b5c9734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Installments Payments\" final shape: (339587, 85)\n"
     ]
    }
   ],
   "source": [
    "payments=installment(payments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a0671f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 categorical variables\n",
      " \n",
      " []\n"
     ]
    }
   ],
   "source": [
    "catVars = [var for var in payments.columns if payments[var].dtype=='object']\n",
    "\n",
    "print('There are {} categorical variables\\n'.format(len(catVars)), '\\n', catVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9198d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_card(cc):    \n",
    "    \n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category=True)\n",
    "\n",
    "    # Amount used from limit\n",
    "    cc['LIMIT_USE'] = cc['AMT_BALANCE'] / cc['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "    # Current payment / Min payment\n",
    "    cc['PAYMENT_DIV_MIN'] = cc['AMT_PAYMENT_CURRENT'] / cc['AMT_INST_MIN_REGULARITY']\n",
    "    # Late payment <-- 'CARD_IS_DPD'\n",
    "    cc['LATE_PAYMENT'] = cc['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    # How much drawing of limit\n",
    "    cc['DRAWING_LIMIT_RATIO'] = cc['AMT_DRAWINGS_ATM_CURRENT'] / cc['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "\n",
    "    cc['CARD_IS_DPD_UNDER_120'] = cc['SK_DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n",
    "    cc['CARD_IS_DPD_OVER_120'] = cc['SK_DPD'].apply(lambda x: 1 if x >= 120 else 0)\n",
    "\n",
    "    # General aggregations\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "\n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "\n",
    "    # Last month balance of each credit card application\n",
    "    last_ids = cc.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n",
    "    last_months_df = cc[cc.index.isin(last_ids)]\n",
    "    cc_agg = group_and_merge(last_months_df,cc_agg,'CC_LAST_', {'AMT_BALANCE': ['mean', 'max']})\n",
    "\n",
    "    CREDIT_CARD_TIME_AGG = {\n",
    "        'AMT_BALANCE': ['mean', 'max'],\n",
    "        'LIMIT_USE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_LIMIT_ACTUAL':['max'],\n",
    "        'AMT_DRAWINGS_ATM_CURRENT': ['max', 'sum'],\n",
    "        'AMT_DRAWINGS_CURRENT': ['max', 'sum'],\n",
    "        'AMT_DRAWINGS_POS_CURRENT': ['max', 'sum'],\n",
    "        'AMT_INST_MIN_REGULARITY': ['max', 'mean'],\n",
    "        'AMT_PAYMENT_TOTAL_CURRENT': ['max','sum'],\n",
    "        'AMT_TOTAL_RECEIVABLE': ['max', 'mean'],\n",
    "        'CNT_DRAWINGS_ATM_CURRENT': ['max','sum', 'mean'],\n",
    "        'CNT_DRAWINGS_CURRENT': ['max', 'mean', 'sum'],\n",
    "        'CNT_DRAWINGS_POS_CURRENT': ['mean'],\n",
    "        'SK_DPD': ['mean', 'max', 'sum'],\n",
    "        'LIMIT_USE': ['min', 'max'],\n",
    "        'DRAWING_LIMIT_RATIO': ['min', 'max'],\n",
    "        'LATE_PAYMENT': ['mean', 'sum'],\n",
    "        'CARD_IS_DPD_UNDER_120': ['mean', 'sum'],\n",
    "        'CARD_IS_DPD_OVER_120': ['mean', 'sum']\n",
    "    }\n",
    "\n",
    "    for months in [12, 24, 48]:\n",
    "        cc_prev_id = cc[cc['MONTHS_BALANCE'] >= -months]['SK_ID_PREV'].unique()\n",
    "        cc_recent = cc[cc['SK_ID_PREV'].isin(cc_prev_id)]\n",
    "        prefix = 'INS_{}M_'.format(months)\n",
    "        cc_agg = group_and_merge(cc_recent, cc_agg, prefix, CREDIT_CARD_TIME_AGG)\n",
    "\n",
    "\n",
    "    print('\"Credit Card Balance\" final shape:', cc_agg.shape)\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93f6f6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Credit Card Balance\" final shape: (103558, 284)\n"
     ]
    }
   ],
   "source": [
    "cc_bal=credit_card(cc_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaddcd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 categorical variables\n",
      " \n",
      " []\n"
     ]
    }
   ],
   "source": [
    "catVars = [var for var in cc_bal.columns if cc_bal[var].dtype=='object']\n",
    "\n",
    "print('There are {} categorical variables\\n'.format(len(catVars)), '\\n', catVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "298def8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_post_processing(dataframe):\n",
    "    print(f'---=> the DATA POST-PROCESSING is beginning, the dataset has {dataframe.shape[1]} features')\n",
    "    # keep index related columns\n",
    "    index_cols = ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV', 'index']\n",
    "\n",
    "    dataframe = dataframe.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '_', x))\n",
    "    print('names of feature are renamed')\n",
    "\n",
    "    '''# Reduced memory usage\n",
    "    dataframe = reduce_mem_usage(dataframe)\n",
    "    print(f'---=> pandas data types of features in the dataset are converted for a reduced memory usage')\n",
    "'''\n",
    "    # Remove non-informative columns\n",
    "    noninformative_cols = []\n",
    "    for col in dataframe.columns:\n",
    "        if len(dataframe[col].value_counts()) < 2:\n",
    "            noninformative_cols.append(col)\n",
    "\n",
    "    dataframe.drop(noninformative_cols, axis=1, inplace=True)\n",
    "    print(f'---=> {dataframe.shape[1]} features are remained after removing non-informative features')\n",
    "\n",
    "    # Removing features not interesting for classifier\n",
    "    feature_num = dataframe.shape[1]\n",
    "    #this function does not work reason of insufficient memory, I added selected_feature manually!\n",
    "    '''auc_limit = 0.7\n",
    "    sub_m, selected_features = model1(dataframe, n_folds=5)'''\n",
    "    all_features = dataframe.columns.tolist()\n",
    "    #selected_feature_df = pd.read_csv('../input/homecredit-best-subs/removed_cols_lgbm.csv')\n",
    "    #selected_features = selected_feature_df.removed_cols.tolist()\n",
    "    #remained_features = set(all_features).difference(set(selected_features))\n",
    "    #dataframe = dataframe.drop(feat, axis=1, inplace=True)\n",
    "    #dataframe = dataframe[selected_features]\n",
    "    print(f'{feature_num - dataframe.shape[1]} features are eliminated by LightGBM classifier in step I')\n",
    "    print(f'---=> {dataframe.shape[1]} features are remained after removing features not interesting for LightGBM classifier')\n",
    "\n",
    "\n",
    "    # generate new columns with risk_groupanizer\n",
    "    start_feats_num = dataframe.shape[1]\n",
    "    cat_cols = [col for col in dataframe.columns if 3 < len(dataframe[col].value_counts()) < 20 and col not in index_cols]\n",
    "    dataframe, _ = risk_groupanizer(dataframe, column_names=cat_cols, upper_limit_ratio=8.1, lower_limit_ratio=8.1)\n",
    "    print(f'---=> {dataframe.shape[1] - start_feats_num} features are generated with the risk_groupanizer')\n",
    "\n",
    "\n",
    "    # ending message of DATA POST-PROCESSING\n",
    "    print(f'---=> the DATA POST-PROCESSING is ended!, now the dataset has a total {dataframe.shape[1]} features')\n",
    "\n",
    "    gc.collect()\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bc844fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold_LightGBM(df):\n",
    "    print('===============================================', '\\n', '##### the ML in processing...')\n",
    "\n",
    "    # loading predicted result \n",
    "    df_subx = pd.read_csv(r\"C:\\Users\\aksha\\notebooks\\data\\submission.csv\")\n",
    "    df_sub = df_subx[['SK_ID_CURR', 'TARGET']]\n",
    "    df_sub.columns = ['SK_ID_CURR', 'TARGET']\n",
    "     \n",
    "    # split train, and test datasets\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "    # delete main dataframe for saving memory\n",
    "    del df_subx\n",
    "    gc.collect()\n",
    "\n",
    "        # Expand train dataset with two times of test dataset including predicted results\n",
    "    test_df.TARGET = np.where(df_sub.TARGET > 0.75, 1, 0)\n",
    "    train_df = pd.concat([train_df, test_df], axis=0)\n",
    "    train_df = pd.concat([train_df, test_df], axis=0)\n",
    "    train_df = pd.concat([train_df, test_df], axis=0)\n",
    "    print(f'Train shape: {train_df.shape}, test shape: {test_df.shape} are loaded.')\n",
    "\n",
    "    # Cross validation model\n",
    "    folds = KFold(n_splits=6, shuffle=True, random_state=666)\n",
    "\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "\n",
    "    # limit number of feature to only 174!!!\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV']]\n",
    "    \n",
    "    # print final shape of dataset to evaluate by LightGBM\n",
    "    print(f'only {len(feats)} features from a total {train_df.shape[1]} features are used for ML analysis')\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "        clf = LGBMClassifier(nthread=-1,\n",
    "                            #device_type='gpu',\n",
    "                            n_estimators=5000,\n",
    "                            learning_rate=0.01,\n",
    "                            max_depth=11,\n",
    "                            num_leaves=58,\n",
    "                            colsample_bytree=0.613,\n",
    "                            subsample=0.708,\n",
    "                            max_bin=407,\n",
    "                            reg_alpha=3.564,\n",
    "                            reg_lambda=4.930,\n",
    "                            min_child_weight=6,\n",
    "                            min_child_samples=165,\n",
    "                            #keep_training_booster=True,\n",
    "                            silent=-1,\n",
    "                            verbose=-1,)\n",
    "\n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric='auc')\n",
    "\n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
    "\n",
    "    # create submission file\n",
    "    test_df['TARGET'] = sub_preds\n",
    "    test_df[['SK_ID_CURR', 'TARGET']].to_csv('submission.csv', index=False)\n",
    "    print('a submission file is created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3a9a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A\n",
    "def findnan(df):\n",
    "        # Total missing values\n",
    "        val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        ty=df.dtypes\n",
    "        \n",
    "        # Make a table with the results\n",
    "        table = pd.concat([val, val_percent, ty], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        table = table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values', 2: 'Data Type'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        table = table[\n",
    "            table.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print(df.shape)\n",
    "        print(df.columns)\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(table.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8be3453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "969"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df2.merge(cc_bal, how='left', on='SK_ID_CURR')\n",
    "\n",
    "del df2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70e6a760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 categorical variables in Train \n",
      " \n",
      " []\n",
      "(356250, 1173)\n"
     ]
    }
   ],
   "source": [
    "catVars1 = [var for var in df1.columns if df1[var].dtype=='object']\n",
    "\n",
    "print('There are {} categorical variables in Train \\n'.format(len(catVars)), '\\n', catVars)\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fab3d564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356250, 1173)\n",
      "Index(['SK_ID_CURR', 'TARGET', 'CODE_GENDER', 'FLAG_OWN_REALTY',\n",
      "       'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY',\n",
      "       'AMT_GOODS_PRICE', 'REGION_POPULATION_RELATIVE',\n",
      "       ...\n",
      "       'INS_48M_SK_DPD_MAX', 'INS_48M_SK_DPD_SUM',\n",
      "       'INS_48M_DRAWING_LIMIT_RATIO_MIN', 'INS_48M_DRAWING_LIMIT_RATIO_MAX',\n",
      "       'INS_48M_LATE_PAYMENT_MEAN', 'INS_48M_LATE_PAYMENT_SUM',\n",
      "       'INS_48M_CARD_IS_DPD_UNDER_120_MEAN',\n",
      "       'INS_48M_CARD_IS_DPD_UNDER_120_SUM',\n",
      "       'INS_48M_CARD_IS_DPD_OVER_120_MEAN',\n",
      "       'INS_48M_CARD_IS_DPD_OVER_120_SUM'],\n",
      "      dtype='object', length=1173)\n",
      "Your selected dataframe has 1173 columns.\n",
      "There are 1001 columns that have missing values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>% of Total Values</th>\n",
       "      <th>Data Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>REFUSED_DAYS_LAST_DUE_DIFF_MAX</th>\n",
       "      <td>356250</td>\n",
       "      <td>100.0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REFUSED_DAYS_LAST_DUE_DIFF_MEAN</th>\n",
       "      <td>356250</td>\n",
       "      <td>100.0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REFUSED_DAYS_TERMINATION_MAX</th>\n",
       "      <td>356250</td>\n",
       "      <td>100.0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACTIVE_ENDDATE_DIF_MEAN</th>\n",
       "      <td>354347</td>\n",
       "      <td>99.5</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACTIVE_ENDDATE_DIF_MAX</th>\n",
       "      <td>354347</td>\n",
       "      <td>99.5</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANNUITY_INCOME_PERC</th>\n",
       "      <td>36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APP_AMT_INCOME_TOTAL_12_AMT_ANNUITY_ratio</th>\n",
       "      <td>36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INCOME_PER_PERSON</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNT_FAM_MEMBERS</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APPS_CNT_FAM_INCOME_RATIO</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1001 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Missing Values  % of Total Values  \\\n",
       "REFUSED_DAYS_LAST_DUE_DIFF_MAX                     356250              100.0   \n",
       "REFUSED_DAYS_LAST_DUE_DIFF_MEAN                    356250              100.0   \n",
       "REFUSED_DAYS_TERMINATION_MAX                       356250              100.0   \n",
       "ACTIVE_ENDDATE_DIF_MEAN                            354347               99.5   \n",
       "ACTIVE_ENDDATE_DIF_MAX                             354347               99.5   \n",
       "...                                                   ...                ...   \n",
       "ANNUITY_INCOME_PERC                                    36                0.0   \n",
       "APP_AMT_INCOME_TOTAL_12_AMT_ANNUITY_ratio              36                0.0   \n",
       "INCOME_PER_PERSON                                       2                0.0   \n",
       "CNT_FAM_MEMBERS                                         2                0.0   \n",
       "APPS_CNT_FAM_INCOME_RATIO                               2                0.0   \n",
       "\n",
       "                                          Data Type  \n",
       "REFUSED_DAYS_LAST_DUE_DIFF_MAX              float64  \n",
       "REFUSED_DAYS_LAST_DUE_DIFF_MEAN             float64  \n",
       "REFUSED_DAYS_TERMINATION_MAX                float64  \n",
       "ACTIVE_ENDDATE_DIF_MEAN                     float64  \n",
       "ACTIVE_ENDDATE_DIF_MAX                      float64  \n",
       "...                                             ...  \n",
       "ANNUITY_INCOME_PERC                         float64  \n",
       "APP_AMT_INCOME_TOTAL_12_AMT_ANNUITY_ratio   float64  \n",
       "INCOME_PER_PERSON                           float64  \n",
       "CNT_FAM_MEMBERS                             float64  \n",
       "APPS_CNT_FAM_INCOME_RATIO                   float64  \n",
       "\n",
       "[1001 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findnan(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1963560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 77 columns with greater than 75% missing values.\n",
      "(356250, 1096)\n",
      "Index(['SK_ID_CURR', 'TARGET', 'CODE_GENDER', 'FLAG_OWN_REALTY',\n",
      "       'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY',\n",
      "       'AMT_GOODS_PRICE', 'REGION_POPULATION_RELATIVE',\n",
      "       ...\n",
      "       'INS_48M_CNT_DRAWINGS_CURRENT_SUM', 'INS_48M_SK_DPD_MEAN',\n",
      "       'INS_48M_SK_DPD_MAX', 'INS_48M_SK_DPD_SUM', 'INS_48M_LATE_PAYMENT_MEAN',\n",
      "       'INS_48M_LATE_PAYMENT_SUM', 'INS_48M_CARD_IS_DPD_UNDER_120_MEAN',\n",
      "       'INS_48M_CARD_IS_DPD_UNDER_120_SUM',\n",
      "       'INS_48M_CARD_IS_DPD_OVER_120_MEAN',\n",
      "       'INS_48M_CARD_IS_DPD_OVER_120_SUM'],\n",
      "      dtype='object', length=1096)\n",
      "Your selected dataframe has 1096 columns.\n",
      "There are 924 columns that have missing values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>% of Total Values</th>\n",
       "      <th>Data Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACTIVE_AMT_ANNUITY_MEAN</th>\n",
       "      <td>263524</td>\n",
       "      <td>74.0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACTIVE_AMT_ANNUITY_MAX</th>\n",
       "      <td>263524</td>\n",
       "      <td>74.0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLOSED_AMT_ANNUITY_MAX</th>\n",
       "      <td>261053</td>\n",
       "      <td>73.3</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLOSED_AMT_ANNUITY_MEAN</th>\n",
       "      <td>261053</td>\n",
       "      <td>73.3</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC_LIMIT_USE_VAR</th>\n",
       "      <td>255107</td>\n",
       "      <td>71.6</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <td>36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APP_AMT_INCOME_TOTAL_12_AMT_ANNUITY_ratio</th>\n",
       "      <td>36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INCOME_PER_PERSON</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNT_FAM_MEMBERS</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APPS_CNT_FAM_INCOME_RATIO</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>924 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Missing Values  % of Total Values  \\\n",
       "ACTIVE_AMT_ANNUITY_MEAN                            263524               74.0   \n",
       "ACTIVE_AMT_ANNUITY_MAX                             263524               74.0   \n",
       "CLOSED_AMT_ANNUITY_MAX                             261053               73.3   \n",
       "CLOSED_AMT_ANNUITY_MEAN                            261053               73.3   \n",
       "CC_LIMIT_USE_VAR                                   255107               71.6   \n",
       "...                                                   ...                ...   \n",
       "AMT_ANNUITY                                            36                0.0   \n",
       "APP_AMT_INCOME_TOTAL_12_AMT_ANNUITY_ratio              36                0.0   \n",
       "INCOME_PER_PERSON                                       2                0.0   \n",
       "CNT_FAM_MEMBERS                                         2                0.0   \n",
       "APPS_CNT_FAM_INCOME_RATIO                               2                0.0   \n",
       "\n",
       "                                          Data Type  \n",
       "ACTIVE_AMT_ANNUITY_MEAN                     float64  \n",
       "ACTIVE_AMT_ANNUITY_MAX                      float64  \n",
       "CLOSED_AMT_ANNUITY_MAX                      float64  \n",
       "CLOSED_AMT_ANNUITY_MEAN                     float64  \n",
       "CC_LIMIT_USE_VAR                            float64  \n",
       "...                                             ...  \n",
       "AMT_ANNUITY                                 float64  \n",
       "APP_AMT_INCOME_TOTAL_12_AMT_ANNUITY_ratio   float64  \n",
       "INCOME_PER_PERSON                           float64  \n",
       "CNT_FAM_MEMBERS                             float64  \n",
       "APPS_CNT_FAM_INCOME_RATIO                   float64  \n",
       "\n",
       "[924 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = remove_missing_columns(df1)\n",
    "findnan(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b424c7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307506, 1096)\n",
      "(48744, 1096)\n"
     ]
    }
   ],
   "source": [
    "train_df = df1[df1['TARGET'].notnull()]\n",
    "test_df = df1[df1['TARGET'].isnull()]\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4e7966f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>...</th>\n",
       "      <th>INS_48M_CNT_DRAWINGS_CURRENT_SUM</th>\n",
       "      <th>INS_48M_SK_DPD_MEAN</th>\n",
       "      <th>INS_48M_SK_DPD_MAX</th>\n",
       "      <th>INS_48M_SK_DPD_SUM</th>\n",
       "      <th>INS_48M_LATE_PAYMENT_MEAN</th>\n",
       "      <th>INS_48M_LATE_PAYMENT_SUM</th>\n",
       "      <th>INS_48M_CARD_IS_DPD_UNDER_120_MEAN</th>\n",
       "      <th>INS_48M_CARD_IS_DPD_UNDER_120_SUM</th>\n",
       "      <th>INS_48M_CARD_IS_DPD_OVER_120_MEAN</th>\n",
       "      <th>INS_48M_CARD_IS_DPD_OVER_120_SUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>307506</th>\n",
       "      <td>100001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>568800.0</td>\n",
       "      <td>20560.5</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>0.018850</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307507</th>\n",
       "      <td>100005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99000.0</td>\n",
       "      <td>222768.0</td>\n",
       "      <td>17370.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>0.035792</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307508</th>\n",
       "      <td>100013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>663264.0</td>\n",
       "      <td>69777.0</td>\n",
       "      <td>630000.0</td>\n",
       "      <td>0.019101</td>\n",
       "      <td>...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307509</th>\n",
       "      <td>100028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>1575000.0</td>\n",
       "      <td>49018.5</td>\n",
       "      <td>1575000.0</td>\n",
       "      <td>0.026392</td>\n",
       "      <td>...</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307510</th>\n",
       "      <td>100038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>625500.0</td>\n",
       "      <td>32067.0</td>\n",
       "      <td>625500.0</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1096 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SK_ID_CURR  TARGET  CODE_GENDER  FLAG_OWN_REALTY  CNT_CHILDREN  \\\n",
       "307506      100001     NaN            1                0             0   \n",
       "307507      100005     NaN            0                0             0   \n",
       "307508      100013     NaN            0                0             0   \n",
       "307509      100028     NaN            1                0             2   \n",
       "307510      100038     NaN            0                1             1   \n",
       "\n",
       "        AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  AMT_GOODS_PRICE  \\\n",
       "307506          135000.0    568800.0      20560.5         450000.0   \n",
       "307507           99000.0    222768.0      17370.0         180000.0   \n",
       "307508          202500.0    663264.0      69777.0         630000.0   \n",
       "307509          315000.0   1575000.0      49018.5        1575000.0   \n",
       "307510          180000.0    625500.0      32067.0         625500.0   \n",
       "\n",
       "        REGION_POPULATION_RELATIVE  ...  INS_48M_CNT_DRAWINGS_CURRENT_SUM  \\\n",
       "307506                    0.018850  ...                               NaN   \n",
       "307507                    0.035792  ...                               NaN   \n",
       "307508                    0.019101  ...                              23.0   \n",
       "307509                    0.026392  ...                             117.0   \n",
       "307510                    0.010032  ...                               NaN   \n",
       "\n",
       "        INS_48M_SK_DPD_MEAN  INS_48M_SK_DPD_MAX  INS_48M_SK_DPD_SUM  \\\n",
       "307506                  NaN                 NaN                 NaN   \n",
       "307507                  NaN                 NaN                 NaN   \n",
       "307508             0.010417                 1.0                 1.0   \n",
       "307509             0.000000                 0.0                 0.0   \n",
       "307510                  NaN                 NaN                 NaN   \n",
       "\n",
       "        INS_48M_LATE_PAYMENT_MEAN  INS_48M_LATE_PAYMENT_SUM  \\\n",
       "307506                        NaN                       NaN   \n",
       "307507                        NaN                       NaN   \n",
       "307508                   0.010417                       1.0   \n",
       "307509                   0.000000                       0.0   \n",
       "307510                        NaN                       NaN   \n",
       "\n",
       "        INS_48M_CARD_IS_DPD_UNDER_120_MEAN  INS_48M_CARD_IS_DPD_UNDER_120_SUM  \\\n",
       "307506                                 NaN                                NaN   \n",
       "307507                                 NaN                                NaN   \n",
       "307508                            0.010417                                1.0   \n",
       "307509                            0.000000                                0.0   \n",
       "307510                                 NaN                                NaN   \n",
       "\n",
       "        INS_48M_CARD_IS_DPD_OVER_120_MEAN  INS_48M_CARD_IS_DPD_OVER_120_SUM  \n",
       "307506                                NaN                               NaN  \n",
       "307507                                NaN                               NaN  \n",
       "307508                                0.0                               0.0  \n",
       "307509                                0.0                               0.0  \n",
       "307510                                NaN                               NaN  \n",
       "\n",
       "[5 rows x 1096 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7e527726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 19911, number of negative: 226093\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.988665 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 163734\n",
      "[LightGBM] [Info] Number of data points in the train set: 246004, number of used features: 1081\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 19889, number of negative: 226116\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.983238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 163870\n",
      "[LightGBM] [Info] Number of data points in the train set: 246005, number of used features: 1082\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 19858, number of negative: 226147\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.032488 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 163855\n",
      "[LightGBM] [Info] Number of data points in the train set: 246005, number of used features: 1082\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 19841, number of negative: 226164\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.138948 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 163723\n",
      "[LightGBM] [Info] Number of data points in the train set: 246005, number of used features: 1082\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 19797, number of negative: 226208\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.979273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 163899\n",
      "[LightGBM] [Info] Number of data points in the train set: 246005, number of used features: 1082\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    }
   ],
   "source": [
    "sub, feat = model1(df1, n_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7b5c0d",
   "metadata": {},
   "source": [
    "\n",
    "df = df.merge(bureau, how='left', on='SK_ID_CURR')\n",
    "df = df.merge(prev_app, how='left', on='SK_ID_CURR')\n",
    "df = df.merge(cash, how='left', on='SK_ID_CURR')\n",
    "df = df.merge(payments, how='left', on='SK_ID_CURR')\n",
    "'''test1 = test1.merge(bureau, how='left', on='SK_ID_CURR')\n",
    "print('--=> Train after merge with bureau:', train.shape)\n",
    "print('--=> Test after merge with bureau:', test.shape)\n",
    "\n",
    "catVars = [var for var in train1.columns if train1[var].dtype=='object']\n",
    "catVars1 = [var for var in test1.columns if test1[var].dtype=='object']\n",
    "\n",
    "print('There are {} categorical variables in Train \\n'.format(len(catVars)), '\\n', catVars)\n",
    "print('There are {} categorical variables in Test \\n'.format(len(catVars1)), '\\n', catVars1)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64796c2b",
   "metadata": {},
   "source": [
    "\n",
    "'''test1 = test1.merge(prev_app, how='left', on='SK_ID_CURR')\n",
    "print('--=> Train after merge with Previous Applications:', train.shape)\n",
    "print('--=> Test after merge with Previous Applications:', test.shape)\n",
    "\n",
    "catVars = [var for var in train1.columns if train1[var].dtype=='object']\n",
    "catVars1 = [var for var in test1.columns if test1[var].dtype=='object']\n",
    "\n",
    "print('There are {} categorical variables in Train \\n'.format(len(catVars)), '\\n', catVars)\n",
    "print('There are {} categorical variables in Test \\n'.format(len(catVars1)), '\\n', catVars1)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be6e71",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "'''test1 = test1.merge(cash, how='left', on='SK_ID_CURR')\n",
    "print('--=> Train after merge with POS_CASH:', train1.shape)\n",
    "print('--=> Test after merge with POS_CASH:', test1.shape)\n",
    "\n",
    "catVars = [var for var in train1.columns if train1[var].dtype=='object']\n",
    "catVars1 = [var for var in test1.columns if test1[var].dtype=='object']\n",
    "\n",
    "print('There are {} categorical variables in Train \\n'.format(len(catVars)), '\\n', catVars)\n",
    "print('There are {} categorical variables in Test \\n'.format(len(catVars1)), '\\n', catVars1)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffafa6ca",
   "metadata": {},
   "source": [
    "\n",
    "'''test1 = test1.merge(payments, how='left', on='SK_ID_CURR')\n",
    "print('--=> Train after merge with Installment Payments:', train1.shape)\n",
    "print('--=> Test after merge with Installment Payments', test1.shape)\n",
    "\n",
    "catVars = [var for var in train1.columns if train1[var].dtype=='object']\n",
    "catVars1 = [var for var in test1.columns if test1[var].dtype=='object']\n",
    "\n",
    "print('There are {} categorical variables in Train \\n'.format(len(catVars)), '\\n', catVars)\n",
    "print('There are {} categorical variables in Test \\n'.format(len(catVars1)), '\\n', catVars1)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e39796",
   "metadata": {},
   "source": [
    "cc_bal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f5f72",
   "metadata": {},
   "source": [
    "del temp1, temp2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7c6eda",
   "metadata": {},
   "source": [
    "findnan(train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5241ea1",
   "metadata": {},
   "source": [
    "df1 = df.merge(cc_bal, how='left', on='SK_ID_CURR')\n",
    "print('--=> Df after merge with Credit Card:', df.shape)\n",
    "catVars = [var for var in df1.columns if df1[var].dtype=='object']\n",
    "print('There are {} categorical variables in Train \\n'.format(len(catVars)), '\\n', catVars)\n",
    "\n",
    "'''test2 = test1.merge(cc_bal, how='left', on='SK_ID_CURR')\n",
    "print('--=> Train after merge with Credit Card:', train2.shape)\n",
    "print('--=> Test after merge with Credit Card:', test2.shape)\n",
    "\n",
    "catVars = [var for var in train2.columns if train2[var].dtype=='object']\n",
    "catVars1 = [var for var in test2.columns if test2[var].dtype=='object']\n",
    "\n",
    "print('There are {} categorical variables in Train \\n'.format(len(catVars)), '\\n', catVars)\n",
    "print('There are {} categorical variables in Test \\n'.format(len(catVars1)), '\\n', catVars1)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28332192",
   "metadata": {},
   "source": [
    "findnan(train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cfe521",
   "metadata": {},
   "source": [
    "#findnan(ftrain)\n",
    "\n",
    "print('--=> Train after merge with Credit Card:', df.shape)\n",
    "#print('--=> Test after merge with Credit Card:', ftest.shape)\n",
    "\n",
    "catVars = [var for var in df.columns if df[var].dtype=='object']\n",
    "#catVars1 = [var for var in ftest.columns if ftest[var].dtype=='object']\n",
    "\n",
    "print('There are {} categorical variables in Train \\n'.format(len(catVars)), '\\n', catVars)\n",
    "#print('There are {} categorical variables in Test \\n'.format(len(catVars1)), '\\n', catVars1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580f51fa",
   "metadata": {},
   "source": [
    "for col in catVars:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce').convert_dtypes() \n",
    "    \n",
    "'''for col in catVars1:\n",
    "    ftest[col] = pd.to_numeric(ftest[col], errors='coerce').convert_dtypes() \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb9c6b1",
   "metadata": {},
   "source": [
    "df1 = remove_missing_columns(df1)\n",
    "findnan(df1)\n",
    "\n",
    "'''del df, bureau, bureau_bal, train, test, cash, prev_app, cc_bal, payments\n",
    "gc.collect()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ee810b",
   "metadata": {},
   "source": [
    "\n",
    "train = train.merge(cash, how='left', on='SK_ID_CURR')\n",
    "test = test.merge(cash, how='left', on='SK_ID_CURR')\n",
    "print('--=> Train after merge with POS_CASH:', train.shape)\n",
    "print('--=> Test after merge with POS_CASH:', test.shape)\n",
    "train = train.merge(payments, how='left', on='SK_ID_CURR')\n",
    "test = test.merge(payments, how='left', on='SK_ID_CURR')\n",
    "print('--=> Train after merge with Installment Payments:', train.shape)\n",
    "print('--=> Test after merge with Installment Payments', test.shape)\n",
    "train = train.merge(cc_bal, how='left', on='SK_ID_CURR')\n",
    "test = test.merge(cc_bal, how='left', on='SK_ID_CURR')\n",
    "print('--=> Train after merge with Credit Card:', train.shape)\n",
    "print('--=> Test after merge with Credit Card:', test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d1895b",
   "metadata": {},
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0ed7b9",
   "metadata": {},
   "source": [
    "catVars = [var for var in df1.columns if df1[var].dtype=='object']\n",
    "\n",
    "print('There are {} categorical variables\\n'.format(len(catVars)), '\\n', catVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "40693710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>...</th>\n",
       "      <th>CC_LIMIT_USE_MIN</th>\n",
       "      <th>CC_LIMIT_USE_MAX</th>\n",
       "      <th>CC_LIMIT_USE_MEAN</th>\n",
       "      <th>CC_LIMIT_USE_SUM</th>\n",
       "      <th>CC_LIMIT_USE_VAR</th>\n",
       "      <th>CC_PAYMENT_DIV_MIN_SUM</th>\n",
       "      <th>CC_LATE_PAYMENT_SUM</th>\n",
       "      <th>CC_DRAWING_LIMIT_RATIO_SUM</th>\n",
       "      <th>CC_LAST_AMT_BALANCE_MEAN</th>\n",
       "      <th>CC_LAST_AMT_BALANCE_MAX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>0.018801</td>\n",
       "      <td>-9461</td>\n",
       "      <td>-637.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>-16765</td>\n",
       "      <td>-1188.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>-19046</td>\n",
       "      <td>-225.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>-19005</td>\n",
       "      <td>-3039.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>0.028663</td>\n",
       "      <td>-19932</td>\n",
       "      <td>-3038.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 629 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET  CODE_GENDER  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0      100002     1.0            0          202500.0    406597.5      24700.5   \n",
       "1      100003     0.0            1          270000.0   1293502.5      35698.5   \n",
       "2      100004     0.0            0           67500.0    135000.0       6750.0   \n",
       "3      100006     0.0            1          135000.0    312682.5      29686.5   \n",
       "4      100007     0.0            0          121500.0    513000.0      21865.5   \n",
       "\n",
       "   AMT_GOODS_PRICE  REGION_POPULATION_RELATIVE  DAYS_BIRTH  DAYS_EMPLOYED  \\\n",
       "0         351000.0                    0.018801       -9461         -637.0   \n",
       "1        1129500.0                    0.003541      -16765        -1188.0   \n",
       "2         135000.0                    0.010032      -19046         -225.0   \n",
       "3         297000.0                    0.008019      -19005        -3039.0   \n",
       "4         513000.0                    0.028663      -19932        -3038.0   \n",
       "\n",
       "   ...  CC_LIMIT_USE_MIN  CC_LIMIT_USE_MAX  CC_LIMIT_USE_MEAN  \\\n",
       "0  ...               NaN               NaN                NaN   \n",
       "1  ...               NaN               NaN                NaN   \n",
       "2  ...               NaN               NaN                NaN   \n",
       "3  ...               0.0               0.0                0.0   \n",
       "4  ...               NaN               NaN                NaN   \n",
       "\n",
       "   CC_LIMIT_USE_SUM  CC_LIMIT_USE_VAR  CC_PAYMENT_DIV_MIN_SUM  \\\n",
       "0               NaN               NaN                     NaN   \n",
       "1               NaN               NaN                     NaN   \n",
       "2               NaN               NaN                     NaN   \n",
       "3               0.0               0.0                     0.0   \n",
       "4               NaN               NaN                     NaN   \n",
       "\n",
       "   CC_LATE_PAYMENT_SUM  CC_DRAWING_LIMIT_RATIO_SUM  CC_LAST_AMT_BALANCE_MEAN  \\\n",
       "0                  NaN                         NaN                       NaN   \n",
       "1                  NaN                         NaN                       NaN   \n",
       "2                  NaN                         NaN                       NaN   \n",
       "3                  0.0                         0.0                       0.0   \n",
       "4                  NaN                         NaN                       NaN   \n",
       "\n",
       "   CC_LAST_AMT_BALANCE_MAX  \n",
       "0                      NaN  \n",
       "1                      NaN  \n",
       "2                      NaN  \n",
       "3                      0.0  \n",
       "4                      NaN  \n",
       "\n",
       "[5 rows x 629 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e53b7219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---=> the DATA POST-PROCESSING is beginning, the dataset has 629 features\n",
      "names of feature are renamed\n",
      "---=> 629 features are remained after removing non-informative features\n",
      "0 features are eliminated by LightGBM classifier in step I\n",
      "---=> 629 features are remained after removing features not interesting for LightGBM classifier\n",
      "---=> 18 features are generated with the risk_groupanizer\n",
      "---=> the DATA POST-PROCESSING is ended!, now the dataset has a total 647 features\n",
      "================================================== \n",
      "\n",
      "---=> Train final shape: (356250, 647)  <=--- \n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "df2 = data_post_processing(df1)\n",
    "\n",
    "print('='*50, '\\n')\n",
    "print('---=> Train final shape:', df2.shape, ' <=---', '\\n')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52f2628",
   "metadata": {},
   "source": [
    "df2,col=one_hot_encoder(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a98ccf8",
   "metadata": {},
   "source": [
    "findnan(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0d3773f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================== \n",
      " ##### the ML in processing...\n",
      "Train shape: (453738, 647), test shape: (48744, 647) are loaded.\n",
      "only 645 features from a total 647 features are used for ML analysis\n",
      "Fold  1 AUC : 0.879417\n",
      "Fold  2 AUC : 0.876873\n",
      "Fold  3 AUC : 0.879510\n",
      "Fold  4 AUC : 0.874981\n",
      "Fold  5 AUC : 0.877054\n",
      "Fold  6 AUC : 0.881072\n",
      "Full AUC score 0.878152\n",
      "a submission file is created\n",
      "--=> all calculations are done!! <=--\n"
     ]
    }
   ],
   "source": [
    "Kfold_LightGBM(df2)\n",
    "print('--=> all calculations are done!! <=--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f38663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
